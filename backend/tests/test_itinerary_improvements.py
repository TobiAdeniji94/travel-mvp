#!/usr/bin/env python3
"""
Test script to demonstrate the improvements made to the itinerary generation system.
This script tests various aspects of the improved code.
"""

import asyncio
import logging
from datetime import datetime, timedelta
from unittest.mock import Mock, AsyncMock, patch
import pytest

# Configure logging for tests
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TestItineraryImprovements:
    """Test class to demonstrate the improvements made to the itinerary system."""
    
    def test_input_validation(self):
        """Test that input validation works correctly."""
        from app.api.schemas import ItineraryCreate
        
        # Test valid input
        valid_request = ItineraryCreate(text="Plan a 3-day trip to Paris")
        assert valid_request.text == "Plan a 3-day trip to Paris"
        
        # Test empty input
        with pytest.raises(ValueError, match="Travel request cannot be empty"):
            ItineraryCreate(text="")
        
        # Test too long input
        long_text = "x" * 2001
        with pytest.raises(ValueError, match="Travel request too long"):
            ItineraryCreate(text=long_text)
        
        # Test malicious input
        malicious_text = "Plan a trip <script>alert('xss')</script>"
        with pytest.raises(ValueError, match="Travel request contains invalid content"):
            ItineraryCreate(text=malicious_text)
    
    def test_error_handling(self):
        """Test that error handling works correctly."""
        from app.api.itinerary import parse_opening_hours
        
        # Test valid opening hours
        result = parse_opening_hours("09:00-17:00")
        assert result == (datetime.strptime("09:00", "%H:%M").time(), 
                         datetime.strptime("17:00", "%H:%M").time())
        
        # Test invalid opening hours (should return default)
        result = parse_opening_hours("invalid")
        assert result == (datetime.strptime("09:00", "%H:%M").time(), 
                         datetime.strptime("17:00", "%H:%M").time())
    
    @pytest.mark.asyncio
    async def test_performance_timer(self):
        """Test that performance timing works correctly."""
        from app.api.itinerary import performance_timer
        
        async with performance_timer("test_operation"):
            await asyncio.sleep(0.1)  # Simulate some work
        
        # The timer should log the operation duration
        # This is a basic test - in a real scenario you'd check the logs
    
    def test_ml_model_loading(self):
        """Test ML model loading with error handling."""
        from app.api.itinerary import load_ml_models
        
        # Test with non-existent files (should raise exception)
        with patch('builtins.open', side_effect=FileNotFoundError("Model not found")):
            with pytest.raises(Exception):
                load_ml_models()
    
    def test_settings_configuration(self):
        """Test that settings are properly configured."""
        from app.core.settings import Settings
        
        settings = Settings()
        
        # Test default values
        assert settings.MAX_ITINERARY_DAYS == 30
        assert settings.DEFAULT_RADIUS_KM == 20
        assert settings.DEFAULT_BUDGET == 1000.0
        assert settings.MAX_REQUEST_LENGTH == 2000
        assert settings.ENABLE_RATE_LIMITING == True
        assert settings.RATE_LIMIT_GENERATE == "5/minute"
    
    @pytest.mark.asyncio
    async def test_itinerary_service_structure(self):
        """Test that the ItineraryService class is properly structured."""
        from app.api.itinerary import ItineraryService
        from sqlalchemy.ext.asyncio import AsyncSession
        
        # Mock session
        mock_session = AsyncMock(spec=AsyncSession)
        
        # Create service instance
        service = ItineraryService(mock_session)
        assert service.session == mock_session
        
        # Test that service has expected methods
        assert hasattr(service, 'parse_travel_request')
        assert hasattr(service, 'process_dates')
        assert hasattr(service, 'get_location_coordinates')
        assert hasattr(service, 'build_poi_list')
        assert hasattr(service, 'create_itinerary_schedule')
        assert hasattr(service, 'persist_itinerary')
    
    def test_logging_configuration(self):
        """Test that logging is properly configured."""
        import logging
        
        # Check that our logger exists
        logger = logging.getLogger('app.api.itinerary')
        assert logger is not None
        
        # Test that we can log messages
        logger.info("Test log message")
        # In a real test, you'd verify the log output
    
    def test_rate_limiting_configuration(self):
        """Test that rate limiting is properly configured."""
        from app.api.itinerary import limiter
        
        # Check that limiter is initialized
        assert limiter is not None
        
        # Test rate limit decorators exist
        # This is a basic test - in a real scenario you'd test the actual rate limiting

def run_improvement_demo():
    """Run a demonstration of the improvements."""
    print("üöÄ Travel MVP API Improvements Demo")
    print("=" * 50)
    
    print("\n‚úÖ Improvements Implemented:")
    print("1. ‚úÖ Proper Error Handling & Logging")
    print("   - Structured logging with context")
    print("   - Performance timing for operations")
    print("   - Specific exception handling")
    
    print("\n2. ‚úÖ Code Organization")
    print("   - Broke down monolithic function into ItineraryService class")
    print("   - Separated concerns (parsing, scheduling, persistence)")
    print("   - Better maintainability and testability")
    
    print("\n3. ‚úÖ Input Validation & Security")
    print("   - Pydantic validators for request data")
    print("   - Malicious content detection")
    print("   - Length and format validation")
    
    print("\n4. ‚úÖ Rate Limiting")
    print("   - 5 requests/minute for itinerary generation")
    print("   - Different limits for different operations")
    print("   - Prevents API abuse")
    
    print("\n5. ‚úÖ Configuration Management")
    print("   - Centralized settings in Settings class")
    print("   - Environment-based configuration")
    print("   - Configurable rate limits and limits")
    
    print("\n6. ‚úÖ Better API Documentation")
    print("   - Detailed response models")
    print("   - Proper HTTP status codes")
    print("   - Comprehensive error responses")
    
    print("\n7. ‚úÖ Performance Monitoring")
    print("   - Operation timing")
    print("   - Structured logging")
    print("   - Health check endpoints")
    
    print("\nüìä Key Benefits:")
    print("- üîí Better security with input validation")
    print("- üöÄ Improved performance with caching and optimization")
    print("- üõ°Ô∏è Protection against abuse with rate limiting")
    print("- üìù Better debugging with structured logging")
    print("- üß™ Easier testing with service layer separation")
    print("- ‚öôÔ∏è Flexible configuration management")
    print("- üìö Better API documentation and error handling")
    
    print("\nüéØ Next Steps:")
    print("1. Add comprehensive unit tests")
    print("2. Implement caching layer (Redis)")
    print("3. Add monitoring and alerting")
    print("4. Implement database connection pooling")
    print("5. Add API versioning")
    print("6. Implement request/response compression")

if __name__ == "__main__":
    run_improvement_demo() 